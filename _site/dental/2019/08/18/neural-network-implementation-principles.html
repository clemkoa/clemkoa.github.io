<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Neural network implementation principles | Clement Joudet - blog</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Neural network implementation principles" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="It’s easy to get lost or discouraged while implementing a neural net, especially from state-of-the-art papers. These models are complex, and I found myself stuck many times while trying to train one of them. Over time, I developed a few principles that help me implement neural net better and faster." />
<meta property="og:description" content="It’s easy to get lost or discouraged while implementing a neural net, especially from state-of-the-art papers. These models are complex, and I found myself stuck many times while trying to train one of them. Over time, I developed a few principles that help me implement neural net better and faster." />
<link rel="canonical" href="http://localhost:4000/dental/2019/08/18/neural-network-implementation-principles.html" />
<meta property="og:url" content="http://localhost:4000/dental/2019/08/18/neural-network-implementation-principles.html" />
<meta property="og:site_name" content="Clement Joudet - blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-18T00:00:00+10:00" />
<script type="application/ld+json">
{"headline":"Neural network implementation principles","dateModified":"2019-08-18T00:00:00+10:00","datePublished":"2019-08-18T00:00:00+10:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/dental/2019/08/18/neural-network-implementation-principles.html"},"url":"http://localhost:4000/dental/2019/08/18/neural-network-implementation-principles.html","description":"It’s easy to get lost or discouraged while implementing a neural net, especially from state-of-the-art papers. These models are complex, and I found myself stuck many times while trying to train one of them. Over time, I developed a few principles that help me implement neural net better and faster.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Clement Joudet - blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Clement Joudet - blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Neural network implementation principles</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-08-18T00:00:00+10:00" itemprop="datePublished">Aug 18, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>It’s easy to get lost or discouraged while implementing a neural net, especially from state-of-the-art papers. These models are complex, and I found myself stuck many times while trying to train one of them. Over time, I developed a few principles that help me implement neural net better and faster.</p>

<p>The type of task this post addresses is not fine-tuning a deep learning model to gain a few 0.1%, or build a production-ready end-to-end pipeline. It is more about trying to replicate a paper’s results, based on a blurry definition of the model architecture.</p>

<p>Andrej Karpathy released an incredibly nice recipe for NN implementation a few months ago: <a href="https://karpathy.github.io/2019/04/25/recipe/">https://karpathy.github.io/2019/04/25/recipe/</a>. While his post is very thorough, I thought I would add a few big-picture principles and specify where my “recipe” differs from his.</p>

<h2 id="principle-1-reduce-complexity">Principle 1. Reduce complexity.</h2>

<p>This seems very simple and obvious. And it’s what we’re all trying to do every time right? Well, it’s easier if you have a checklist to follow, one thing at a time. The goal here is to actively isolate problems and reduce complexity.</p>

<p>When it comes to implementing neural nets, here is my usual checklist:</p>

<h4 id="1-focus-on-forward-pass-first">1. Focus on forward pass first</h4>

<p>This will help define the architecture and layers, and having a clear idea of the output of the neural network. Some papers can seem blurry about the model architecture, and I believe starting with the forward pass helps setting a proper foundation for the implementation. You will have a clearer view of what the net does, and how the training works.</p>

<p>I also find that forcing myself to write an architecture skeleton raises some important questions early. It really helps understand the key points of the paper.
No need to get lost building the whole data pipeline when you don’t have a clear vision of the model.</p>

<h4 id="2-train-by-overfitting">2. Train by overfitting</h4>

<p>Now you’re ready for back-propagation. While training, error sources are plenty: you can have issues in your dataset, your loss, or any mathematical operation you perform. You can have wrong parameters or initialisation that will prevent the network from learning properly. In order to reduce complexity, it’s important to focus on one thing at time. My plan is usually the following:</p>

<ul>
  <li>Start with only 1 sample that you selected. If the training doesn’t converge, you have big issues. Validate the NN output on the same sample as the training one. This way your training set is also your evaluation set, and you can focus on initialisation and meta-parameters.</li>
  <li>Train with ~10 samples</li>
  <li>Train on whole dataset</li>
</ul>

<p>By starting with only one sample, you reduce complexity coming from the dataset and the data pipeline. Overfitting on one sample will make you focus on the loss definition, back-propagation and initialisation. As soon as training converges on 1 sample, try with a dozen samples. This will introduce a bit of variety in the data distribution, as well as a stronger data pipeline. When the model works on a small subset, you can extend to the whole dataset and focus on the data pipeline aspect.</p>

<h4 id="3-initialisation-is-key">3. Initialisation is key</h4>

<p>Now that your network is able to train on the whole dataset and you can see the loss going down, it’s time to help the learning process.
Sometimes the loss will be stuck around a specific value, and it feels like the network is not able to go past it.
Having a proper look at your network initialisation can save you a lot of time. Wrong initialisation values, too big or too small, can set you back for a while.
I recommend you have a go at <a href="https://www.deeplearning.ai/ai-notes/initialization/">https://www.deeplearning.ai/ai-notes/initialization/</a> for a
very interesting playground.</p>

<h2 id="principle-2-reduce-the-potential-error-sources">Principle 2. Reduce the potential error sources.</h2>

<h4 id="mind-your-transformations">Mind your transformations</h4>

<p>You should be wary of tensor reshaping. It is easy to use a <code class="highlighter-rouge">reshape</code> instead of a <code class="highlighter-rouge">permute</code>, and your network won’t be able to train properly because you mixed up your data across dimensions. In general, you should be careful about any tensor reshaping!</p>

<h4 id="know-you-dataset">Know you dataset</h4>

<p>As Andrej Karpathy says, you need to become one with the data in order to understand what you are doing.</p>

<h4 id="visualize-everything">Visualize everything</h4>

<p>You need to visualize inputs, outputs, training loss, middle layers and any variable that is important to your model. Tensorboard is great for this.</p>

<h4 id="test-everything">Test everything</h4>

<p>NNs rely on mathematical functions and it’s easy to make a mistake / typo in big numpy one-liners. Testing everything will ensure:</p>

<ul>
  <li>You have separated your code in dedicated and unitary functions</li>
  <li>Your functions do exactly what you want.</li>
  <li>You have more confidence in your code and can debug easier</li>
</ul>

<p>In python, pytest is great for testing.</p>

<h4 id="reproducibility-matters">Reproducibility matters</h4>

<p>Vanishing gradient is one of the problems for which it’s nice to reproduce the issue.
Setting the random seed is a mandatory step for any model implementation: run it twice, get the same results.</p>

<h4 id="start-with-code-readability-instead-of-optimisation">Start with code readability instead of optimisation</h4>

<p>You will gain time having a slower but easier to debug model at first. Once your model trains perfectly, you can spend some time on optimising every step.</p>

<h2 id="principle-3-most-of-the-intelligence-is-in-the-definition-of-the-loss">Principle 3. Most of the intelligence is in the definition of the loss.</h2>

<p>The training loss and back-propagation are often the most complex steps in the implementation, because it’s where most of the intelligence comes from. It’s worth spending some time on your loss definition!</p>

<p>Hope you find these simple principles useful for your future implementations.</p>

<hr />
<p>Edited on 2020-05-21</p>

  </div><a class="u-url" href="/dental/2019/08/18/neural-network-implementation-principles.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Clement Joudet - blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Clement Joudet - blog</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/clemkoa"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">clemkoa</span></a></li><li><a href="https://www.twitter.com/Clemkoa"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">Clemkoa</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Personal blog about machine learning, software and tech.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>

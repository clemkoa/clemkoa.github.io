<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-21T22:56:24+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Clement Joudet - blog</title><subtitle>Personal blog about machine learning, software and tech.</subtitle><entry><title type="html">Neural network implementation principles</title><link href="http://localhost:4000/dental/2019/08/18/neural-network-implementation-principles.html" rel="alternate" type="text/html" title="Neural network implementation principles" /><published>2019-08-18T00:00:00+10:00</published><updated>2019-08-18T00:00:00+10:00</updated><id>http://localhost:4000/dental/2019/08/18/neural-network-implementation-principles</id><content type="html" xml:base="http://localhost:4000/dental/2019/08/18/neural-network-implementation-principles.html">&lt;p&gt;It’s easy to get lost or discouraged while implementing a neural net, especially from state-of-the-art papers. These models are complex, and I found myself stuck many times while trying to train one of them. Over time, I developed a few principles that help me implement neural net better and faster.&lt;/p&gt;

&lt;p&gt;The type of task this post addresses is not fine-tuning a deep learning model to gain a few 0.1%, or build a production-ready end-to-end pipeline. It is more about trying to replicate a paper’s results, based on a blurry definition of the model architecture.&lt;/p&gt;

&lt;p&gt;Andrej Karpathy released an incredibly nice recipe for NN implementation a few months ago: &lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/&quot;&gt;https://karpathy.github.io/2019/04/25/recipe/&lt;/a&gt;. While his post is very thorough, I thought I would add a few big-picture principles and specify where my “recipe” differs from his.&lt;/p&gt;

&lt;h2 id=&quot;principle-1-reduce-complexity&quot;&gt;Principle 1. Reduce complexity.&lt;/h2&gt;

&lt;p&gt;This seems very simple and obvious. And it’s what we’re all trying to do every time right? Well, it’s easier if you have a checklist to follow, one thing at a time. The goal here is to actively isolate problems and reduce complexity.&lt;/p&gt;

&lt;p&gt;When it comes to implementing neural nets, here is my usual checklist:&lt;/p&gt;

&lt;h4 id=&quot;1-focus-on-forward-pass-first&quot;&gt;1. Focus on forward pass first&lt;/h4&gt;

&lt;p&gt;This will help define the architecture and layers, and having a clear idea of the output of the neural network. Some papers can seem blurry about the model architecture, and I believe starting with the forward pass helps setting a proper foundation for the implementation. You will have a clearer view of what the net does, and how the training works.&lt;/p&gt;

&lt;p&gt;I also find that forcing myself to write an architecture skeleton raises some important questions early. It really helps understand the key points of the paper.
No need to get lost building the whole data pipeline when you don’t have a clear vision of the model.&lt;/p&gt;

&lt;h4 id=&quot;2-train-by-overfitting&quot;&gt;2. Train by overfitting&lt;/h4&gt;

&lt;p&gt;Now you’re ready for back-propagation. While training, error sources are plenty: you can have issues in your dataset, your loss, or any mathematical operation you perform. You can have wrong parameters or initialisation that will prevent the network from learning properly. In order to reduce complexity, it’s important to focus on one thing at time. My plan is usually the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start with only 1 sample that you selected. If the training doesn’t converge, you have big issues. Validate the NN output on the same sample as the training one. This way your training set is also your evaluation set, and you can focus on initialisation and meta-parameters.&lt;/li&gt;
  &lt;li&gt;Train with ~10 samples&lt;/li&gt;
  &lt;li&gt;Train on whole dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By starting with only one sample, you reduce complexity coming from the dataset and the data pipeline. Overfitting on one sample will make you focus on the loss definition, back-propagation and initialisation. As soon as training converges on 1 sample, try with a dozen samples. This will introduce a bit of variety in the data distribution, as well as a stronger data pipeline. When the model works on a small subset, you can extend to the whole dataset and focus on the data pipeline aspect.&lt;/p&gt;

&lt;h4 id=&quot;3-initialisation-is-key&quot;&gt;3. Initialisation is key&lt;/h4&gt;

&lt;p&gt;Now that your network is able to train on the whole dataset and you can see the loss going down, it’s time to help the learning process.
Sometimes the loss will be stuck around a specific value, and it feels like the network is not able to go past it.
Having a proper look at your network initialisation can save you a lot of time. Wrong initialisation values, too big or too small, can set you back for a while.
I recommend you have a go at &lt;a href=&quot;https://www.deeplearning.ai/ai-notes/initialization/&quot;&gt;https://www.deeplearning.ai/ai-notes/initialization/&lt;/a&gt; for a
very interesting playground.&lt;/p&gt;

&lt;h2 id=&quot;principle-2-reduce-the-potential-error-sources&quot;&gt;Principle 2. Reduce the potential error sources.&lt;/h2&gt;

&lt;h4 id=&quot;mind-your-transformations&quot;&gt;Mind your transformations&lt;/h4&gt;

&lt;p&gt;You should be wary of tensor reshaping. It is easy to use a &lt;code class=&quot;highlighter-rouge&quot;&gt;reshape&lt;/code&gt; instead of a &lt;code class=&quot;highlighter-rouge&quot;&gt;permute&lt;/code&gt;, and your network won’t be able to train properly because you mixed up your data across dimensions. In general, you should be careful about any tensor reshaping!&lt;/p&gt;

&lt;h4 id=&quot;know-you-dataset&quot;&gt;Know you dataset&lt;/h4&gt;

&lt;p&gt;As Andrej Karpathy says, you need to become one with the data in order to understand what you are doing.&lt;/p&gt;

&lt;h4 id=&quot;visualize-everything&quot;&gt;Visualize everything&lt;/h4&gt;

&lt;p&gt;You need to visualize inputs, outputs, training loss, middle layers and any variable that is important to your model. Tensorboard is great for this.&lt;/p&gt;

&lt;h4 id=&quot;test-everything&quot;&gt;Test everything&lt;/h4&gt;

&lt;p&gt;NNs rely on mathematical functions and it’s easy to make a mistake / typo in big numpy one-liners. Testing everything will ensure:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You have separated your code in dedicated and unitary functions&lt;/li&gt;
  &lt;li&gt;Your functions do exactly what you want.&lt;/li&gt;
  &lt;li&gt;You have more confidence in your code and can debug easier&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In python, pytest is great for testing.&lt;/p&gt;

&lt;h4 id=&quot;reproducibility-matters&quot;&gt;Reproducibility matters&lt;/h4&gt;

&lt;p&gt;Vanishing gradient is one of the problems for which it’s nice to reproduce the issue.
Setting the random seed is a mandatory step for any model implementation: run it twice, get the same results.&lt;/p&gt;

&lt;h4 id=&quot;start-with-code-readability-instead-of-optimisation&quot;&gt;Start with code readability instead of optimisation&lt;/h4&gt;

&lt;p&gt;You will gain time having a slower but easier to debug model at first. Once your model trains perfectly, you can spend some time on optimising every step.&lt;/p&gt;

&lt;h2 id=&quot;principle-3-most-of-the-intelligence-is-in-the-definition-of-the-loss&quot;&gt;Principle 3. Most of the intelligence is in the definition of the loss.&lt;/h2&gt;

&lt;p&gt;The training loss and back-propagation are often the most complex steps in the implementation, because it’s where most of the intelligence comes from. It’s worth spending some time on your loss definition!&lt;/p&gt;

&lt;p&gt;Hope you find these simple principles useful for your future implementations.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Edited on 2020-05-21&lt;/p&gt;</content><author><name></name></author><summary type="html">It’s easy to get lost or discouraged while implementing a neural net, especially from state-of-the-art papers. These models are complex, and I found myself stuck many times while trying to train one of them. Over time, I developed a few principles that help me implement neural net better and faster.</summary></entry><entry><title type="html">Improving deep learning object detection on dental x-rays</title><link href="http://localhost:4000/dental/2018/09/06/improving-deep-learning-dental-x-ray.html" rel="alternate" type="text/html" title="Improving deep learning object detection on dental x-rays" /><published>2018-09-06T00:00:00+10:00</published><updated>2018-09-06T00:00:00+10:00</updated><id>http://localhost:4000/dental/2018/09/06/improving-deep-learning-dental-x-ray</id><content type="html" xml:base="http://localhost:4000/dental/2018/09/06/improving-deep-learning-dental-x-ray.html">&lt;p&gt;This is a follow-up on how I improved the model from my previous post.&lt;/p&gt;

&lt;p&gt;What was wrong:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Precision wasn’t great&lt;/li&gt;
  &lt;li&gt;MobileNet (SSD) performed better than Faster RCNN ResNet, which seems counter-intuitive as MobileNet is supposed to be a lighter model&lt;/li&gt;
  &lt;li&gt;Tooth segmentation was not very precise. Often the model would output two teeth as one, and overall you could tell the model wasn’t really precise on the object boxes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article I will talk about two solutions I implemented to increase model accuracy:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image preprocessing&lt;/li&gt;
  &lt;li&gt;Transfer learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-preprocessing&quot;&gt;1. Preprocessing&lt;/h2&gt;

&lt;p&gt;Our x-ray dataset comes from various sources, and as you can see below they vary quite a lot. There are variations in image resolution, size, contrast, and zoom on the teeth. That’s because our x-rays come from different machines used by different radiologists.
Raw input sample&lt;/p&gt;

&lt;p&gt;Image size and resolution variability is not a problem for our model because the images are all very high quality (average size 2900x1400) so it actually downsizes images before processing. Degrees of zoom on the teeth is not a problem either, it actually gives a bit of variety to our model.&lt;/p&gt;

&lt;p&gt;However, gray level and contrast variations can make it hard for our model to really learn the right features.&lt;/p&gt;

&lt;p&gt;A preprocessing step would be to normalize our data to obtain images with similar contrasts and that all “look the same”. Histogram equalization is a good way to increase contrast. But for our data, simple histogram equalization is not good as it can actually reduce the visibility of tooth restorations and implants (see comparison below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-2-preview.png&quot; alt=&quot;assets/dental-2-preview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CLAHE (Contrast Limited Adaptive Histogram Equalization) is a histogram equalization technique that allows to enhance contrast locally while limiting the amplification of noise.&lt;/p&gt;

&lt;p&gt;I used OpenCV, which is an amazing resource for editing images, and can’t recommend it enough. Here is a really nice tutorial on histogram equalization. I was able to simply use CLAHE as follow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;equalize_clahe_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clahe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createCLAHE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clipLimit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tileGridSize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clahe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Empirically, a tile of (16,16) was a good tradeoff for our contrast among all images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-2-clahe.png&quot; alt=&quot;assets/dental-2-clahe.png&quot; /&gt;
&lt;em&gt;&lt;center&gt;Comparison of histogram equalization methods&lt;/center&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-transfer-learning&quot;&gt;2. Transfer learning&lt;/h2&gt;

&lt;p&gt;Tensorflow Object Detection API makes it easy to do transfer learning from an existing model. The model zoo allows you to pick a pre-trained model and easily train it on your dataset.&lt;/p&gt;

&lt;p&gt;I chose to use faster_rcnn_resnet50_coco for its relatively good speed and mAP score on the COCO dataset.&lt;/p&gt;

&lt;p&gt;After 1500 iterations on my laptop, the model is already performing quite well. I used vertical and horizontal flipping for data augmentation. When comparing with my previous model, we can see how much the current model has improved:
&lt;img src=&quot;/assets/dental-2-comparison.png&quot; alt=&quot;assets/dental-2-comparison.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Transfer learning helped for two main reasons. First of all, a pre-trained model makes it easy to learn shapes and specific objects. Secondly, the Faster RCNN ResNet50 uses higher image sizes than my previous models, which probably helped the precision. I also had to lower the IoU threshold for non max suppression from the tensorflow config.&lt;/p&gt;

&lt;p&gt;Current model is still not perfect, especially around implants but it is probably because we don’t have a lot of implant examples in our dataset. I haven’t trained it on GPU which limits the total amount of training possible and makes the current model only a prototype. However the jump in precision indicates that the preprocessing and transfer learning are going in the right direction.&lt;/p&gt;

&lt;p&gt;Here are a few more outputs from the current algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-2-example1.png&quot; alt=&quot;assets/dental-2-example1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-2-example2.png&quot; alt=&quot;assets/dental-2-example2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-2-example3.png&quot; alt=&quot;assets/dental-2-example3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-2-example4.png&quot; alt=&quot;assets/dental-2-example4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training a lot more, on GPU&lt;/li&gt;
  &lt;li&gt;Other models (SSD, bigger ResNets…) comparisons&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can find the code here: &lt;a href=&quot;https://github.com/clemkoa/tooth-detection&quot;&gt;https://github.com/clemkoa/tooth-detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The dataset is not public yet, but we are working on it!&lt;/p&gt;

&lt;p&gt;Disclaimer: all x-rays in the dataset have been anonymized for privacy concerns.&lt;/p&gt;</content><author><name></name></author><summary type="html">This is a follow-up on how I improved the model from my previous post.</summary></entry><entry><title type="html">Deep learning object detection on dental x-rays</title><link href="http://localhost:4000/dental/2018/06/10/deep-learning-dental-x-ray.html" rel="alternate" type="text/html" title="Deep learning object detection on dental x-rays" /><published>2018-06-10T00:00:00+10:00</published><updated>2018-06-10T00:00:00+10:00</updated><id>http://localhost:4000/dental/2018/06/10/deep-learning-dental-x-ray</id><content type="html" xml:base="http://localhost:4000/dental/2018/06/10/deep-learning-dental-x-ray.html">&lt;p&gt;&lt;img src=&quot;/assets/dental-preview.png&quot; alt=&quot;assets/dental-preview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the interest in AI and deep learning keeps growing, we see more and more practical applications, especially in healthcare where breakthroughs could be revolutionary. I teamed up with Dr Arthur Fourcade, an oral surgeon who made his thesis on CNNs applied to medical imagery. Our first project was to work on panoramic dental x-rays and apply a few models as a proof of concept.&lt;/p&gt;

&lt;p&gt;Our goals included:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Build a model that can identify the tooth official index. This is a classification task, and will be the subject of another article.&lt;/li&gt;
  &lt;li&gt;Build a model that can detect restorations, endodontic treatments and implants on a full dental x-ray. This is an object detection task.&lt;/li&gt;
  &lt;li&gt;See how these models compare to humans.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea was not to make a software that would replace doctors at all, but to build a proof of concept that could add a second opinion and could make reporting easier for doctors.&lt;/p&gt;

&lt;p&gt;I chose to use Tensorflow Object Detection API because of its simplicity and plug-and-play approach. The idea was to prototype quickly.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;Dr Arthur Fourcade annotated a dataset of more than 500 dental panoramic x-rays, using the VoTT open-source software. There are other labeling softwares like labellmg, but VoTT was fine for the task we needed. You can export the labels under various formats, including the Tensorflow PASCAL VOC. Dr Fourcade used various datasets of dental x-rays he built during his career. The annotated dataset is not currently open sourced, but will hopefully be soon.&lt;/p&gt;

&lt;p&gt;Dataset features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Every x-ray has been anonymized for privacy protection reasons, there is no way for me to trace back to any individual.&lt;/li&gt;
  &lt;li&gt;More than 500 panoramic x-rays annotated&lt;/li&gt;
  &lt;li&gt;Average size of x-ray: 2900 * 1400 px&lt;/li&gt;
  &lt;li&gt;Classes: 933 endodontic, 2331 restorations and 145 implant occurrences. The number of implant occurrences is a bit low and makes it quite hard for the model to detect implants in an evaluation dataset. This can be countered with data augmentation techniques.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-annotation.png&quot; alt=&quot;assets/dental-annotation.png&quot; /&gt;
&lt;em&gt;&lt;center&gt;Example of annotations on VoTT&lt;/center&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h2&gt;

&lt;p&gt;Preprocessing is the main task before training the model with Tensorflow Object Detection API. You need to save the images and classes under a specific format. The setup tutorial proposed here is very clear and proposes some excellent examples: &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/object_detection&quot;&gt;https://github.com/tensorflow/models/tree/master/research/object_detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can find my pre-processing code here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/clemkoa/tooth-detection/blob/master/data_preprocessing.py&quot;&gt;https://github.com/clemkoa/tooth-detection/blob/master/data_preprocessing.py&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;p&gt;Several data augmentation are needed for this task, but we used mainly vertical and horizontal flips, as well as random black patches and random contrast adjustments.&lt;/p&gt;

&lt;p&gt;I used mainly two models: FastRCNN and SSD_mobilenet_v1. I wanted to compare both models on this very specific task. I used them out of the box and I have made little modifications so far.&lt;/p&gt;

&lt;p&gt;We are dealing with very big images (high resolution) where all the classes are quite similar, that is why an out-of-the-box model might not be fully suited for this task. My goal was to see how well it would perform, to establish a baseline for future improvements. Comparing FastRCNN and SSD was also an objective.&lt;/p&gt;

&lt;p&gt;I started training on my laptop to prototype, but I ended up training everything on Google Cloud platform.&lt;/p&gt;

&lt;p&gt;We chose SSD_mobilenet_v1 for our final model because it ended up giving a better accuracy. Most effort was spent on region proposal features, especially non-max suppression thresholds and number of proposals. As there are 32 teeth max per x-ray, we can fine-tune the region proposal numbers.
Results&lt;/p&gt;

&lt;p&gt;Here are some of the result outputs we have so far:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-example1.png&quot; alt=&quot;assets/dental-example1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-example2.png&quot; alt=&quot;assets/dental-example2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-example3.png&quot; alt=&quot;assets/dental-example3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dental-example4.png&quot; alt=&quot;assets/dental-example4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An example where the model is not working as well as intended&lt;/p&gt;

&lt;p&gt;You can find the code, without the dataset at the moment, here: &lt;a href=&quot;https://github.com/clemkoa/tooth-detection&quot;&gt;https://github.com/clemkoa/tooth-detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are still working on this project, trying to improve accuracy as it was only a prototype. Any feedback is welcome!&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>